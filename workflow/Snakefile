## Phoebe C. R. Parrish
## Berger Lab, FHCRC
## 2022-03-01
## updated 2022-04-21
## snakemake v7.1.0

import os
import sys
import re
import pandas as pd


## set up paths for output files
## - do this automatically for any line containing "dir"?
## - read in directory names as a list/dict => add os.path.abspath as value
fastq_dir = os.path.abspath(config["fastq_dir"]) + "/"
fastq_trimmed_dir = os.path.abspath(config["fastq_trimmed_dir"]) + "/"
fastq_demuxed_dir = os.path.abspath(config["fastq_demuxed_dir"]) + "/"
base_filename = config["base_filename"]
bowtie_idx_dir = os.path.abspath(config["bowtie_idx_dir"]) + "/"
bowtie_idx_ref_dir = os.path.abspath(config["bowtie_idx_ref_dir"]) + "/"
sam_dir = os.path.abspath(config["sam_dir"]) + "/"
bam_dir = os.path.abspath(config["bam_dir"]) + "/"
bam_sorted_dir = os.path.abspath(config["bam_sorted_dir"]) + "/"
pgRNA_counts_dir = os.path.abspath(config["pgRNA_counts_dir"]) + "/"
pgRNA_counts_ref = config["pgRNA_counts_ref"]


## should the below steps be functions or not?
read_to_fastq = {}
fastq_trim_coords = {}

## convert to a dictionary too (so position isn't key)
trim_coords = [config["R1_coords"], config["R2_coords"], config["R3_coords"]]

fastq_fofn = open(config["fastq_fofn"], "r")
i = 0
for fastq in fastq_fofn:
    fastq = fastq.strip()
    key = "R" + str(i+1)
    read_to_fastq[key] = fastq
    fastq_trim_coords[key] = trim_coords[i]

    i += 1
fastq_fofn.close()

sample_list = []
barcode_ref_file = open(config["barcode_ref_file"], "r")
# print("barcode_ref_file=", barcode_ref_file)
for line in barcode_ref_file:
    # print("line=", line)
    ## get sample names from idemp ref file
    sample = line.strip().split("\t")[1]
    # print("sample=", sample)
    sample_list.append(sample)
barcode_ref_file.close()
# print("sample_list=", "sample")

## rule all is used to define output file names for the final rule & for other
##   rules w/ no dependencies
## also important for defining wildcards
rule all:
    input:
        ## rule: build_bowtie_index
        expand(bowtie_idx_dir + "pgPEN_{dmx_read}.{bt_idx}.ebwt",
            dmx_read = ["R1", "R2"],
            bt_idx = ["1", "2", "3", "4", "rev.1", "rev.2"]),
        ## rule: get_stats
        expand(bam_sorted_dir + "flagstat/" + base_filename + "_{dmx_read}_trimmed_{sample}_aligned_sorted_flagstat.txt",
            dmx_read = ["R1", "R2"],
            sample = sample_list),
        ## rule: combine_counts
        pgRNA_counts_dir + base_filename + "_pgRNA_counts.txt"


rule trim_reads:
    input:
        ## try expand instead of lambda - see if it also works!
        # fastq_dir + "PP_pgRNA_HeLa_S1_{read}_001.fastq.gz"
        lambda wildcards: fastq_dir + read_to_fastq[wildcards.read]
    output:
        fastq_trimmed_dir + base_filename + "_{read}_trimmed.fastq"
    conda:
        "envs/fastx_toolkit.yaml"
    params: ## is this the best way to do this?
        start = lambda wildcards: fastq_trim_coords[wildcards.read][0],
        end = lambda wildcards: fastq_trim_coords[wildcards.read][1]
    log:
        "workflow/logs/trim_reads/{read}.log"
    shell:
        "zcat {input} | fastx_trimmer -f {params.start} -l {params.end} -o {output} 2>{log}"


## is this the best way to install/run idemp? (reproducible?)
rule demux_fastqs:
    input:
        idx = fastq_trimmed_dir + base_filename + "_R3_trimmed.fastq",
        R1 = fastq_trimmed_dir + base_filename + "_R1_trimmed.fastq",
        R2 = fastq_trimmed_dir + base_filename + "_R2_trimmed.fastq"
    output:
        ## put the expand statement here so this is only run once
        ##   (instead of once per output file) - can't run w/o expand statement
        ##   because then the input & output don't have matching wildcards
        expand(fastq_demuxed_dir + base_filename + "_{dmx_read}_trimmed_{sample}.fastq.gz",
            dmx_read = ["R1", "R2"],
            sample = sample_list)
    params:
        idemp = config["idemp"],
        n_mismatch = 1,
        ref = config["barcode_ref_file"],
        out_dir = fastq_demuxed_dir, ## can't have folder as output => make param
        bad_names = expand(fastq_demuxed_dir + base_filename + "_{dmx_read}_trimmed.fastq_{sample}.fastq.gz",
            dmx_read = ["R1", "R2"],
            sample = sample_list) ## to get rid of dumb default idemp output names
    log:
        "workflow/logs/demux_fastqs/idemp.log"
    resources:
        mem = 98,
        time = 24,
        cpus_per_task = 16 ## must use Python underscores not Bash dashes
    shell:
        """
        {params.idemp} -b {params.ref} -n {params.n_mismatch} \
        -I1 {input.idx} -R1 {input.R1} -R2 {input.R2} -o {params.out_dir} &> {log}

        ## convert wildcards to bash arrays
        declare -a arr1=({params.bad_names})
        declare -a arr2=({output})
        len=${{#arr1[@]}} ## get length of arrays
        ## loop through wildcard arrays and move to correctly-named file
        for ((i=0; i<$len; i++));
        do
            # echo "${{arr1[$i]}}"
            mv ${{arr1[$i]}} ${{arr2[$i]}}
        done
        """

rule build_bowtie_index:
    input:
        bowtie_idx_ref_dir + "pgPEN_{dmx_read}.fa"
    params:
        out_dir = bowtie_idx_dir + "pgPEN_{dmx_read}"
    output:
        bowtie_idx_dir + "pgPEN_{dmx_read}.{bt_idx}.ebwt"
    conda:
        "envs/bowtie.yaml"
    log:
        "workflow/logs/build_bowtie_index/{dmx_read}_{bt_idx}"
    shell:
        "bowtie-build -f {input} {params.out_dir} 2>{log}"

rule align_reads:
    input:
        fastq = fastq_demuxed_dir + base_filename + "_{dmx_read}_trimmed_{sample}.fastq.gz"
    output:
        sam_dir + base_filename + "_{dmx_read}_trimmed_{sample}_aligned.sam"
    conda:
        "envs/bowtie.yaml"
    params:
        idx = bowtie_idx_dir + "pgPEN_{dmx_read}"
    resources:
        mem = 98,
        time = 24,
        cpus_per_task = 16 ## must use Python underscores not Bash dashes
    log:
        "workflow/logs/align_reads/{dmx_read}_{sample}.log"
    shell:
        "bowtie -q -v 1 --best --strata --all --sam -p 4 {params.idx} {input.fastq} {output} &> {log}"

rule make_sorted_bams:
    input:
        sam_dir + base_filename + "_{dmx_read}_trimmed_{sample}_aligned.sam"
    output:
        bam_sorted_dir + base_filename + "_{dmx_read}_trimmed_{sample}_aligned_sorted.bam"
    conda:
        "envs/samtools.yaml"
    params:
        unsorted_bam_dir = bam_dir,
        unsorted_bam = base_filename + "_{dmx_read}_trimmed_{sample}_aligned.bam",
        tmp = bam_sorted_dir + "tmp_{dmx_read}_{sample}"
    resources:
        mem = 32,
        time = 24
    log:
        "workflow/logs/make_sorted_bams/{dmx_read}_{sample}.log"
    shell:
        """
        mkdir -p {params.unsorted_bam_dir} ## TO DO: write a fxn to make this dir if it doesn't exist

        samtools view -bS -o {params.unsorted_bam_dir}/{params.unsorted_bam} {input}

        samtools sort -O bam -n {params.unsorted_bam_dir}/{params.unsorted_bam} -T {params.tmp} -o {output} &>{log}
        """

rule get_stats:
    input:
        bam_sorted_dir + base_filename + "_{dmx_read}_trimmed_{sample}_aligned_sorted.bam"
    output:
        bam_sorted_dir + "flagstat/" + base_filename + "_{dmx_read}_trimmed_{sample}_aligned_sorted_flagstat.txt"
    conda:
        "envs/samtools.yaml"
    log:
        "workflow/logs/get_stats/{dmx_read}_{sample}.log"
    shell:
        "samtools flagstat {input} 2>{log} 1>{output}" ## fix error issue

rule count_pgRNAs:
    input:
        bam_sorted_dir + base_filename + "_R1_trimmed_{sample}_aligned_sorted.bam"
    output:
        pgRNA_counts_dir + "counts_{sample}.txt"
    conda:
        "envs/counter_efficient.yaml"
    params:
        ref = config["pgRNA_counts_ref"], ## do I need to do os.path.abspath?
        n_chunks = config["n_chunks"], ## default 50 for HeLa
        out_dir = pgRNA_counts_dir,
        script = "workflow/scripts/counter_efficient.R"
    resources:
        mem = 98,
        time = 24,
        cpus_per_task = 16 ## must use Python underscores not Bash dashes
    log:
        "workflow/logs/count_pgRNAs/{sample}.log"
    shell:
        "Rscript {params.script} {input} {params.n_chunks} {params.ref} {params.out_dir} &> {log}"

rule combine_counts:
    input:
        expand(pgRNA_counts_dir + "counts_{sample}.txt",
            sample = sample_list)
    output:
        pgRNA_counts_dir + base_filename + "_pgRNA_counts.txt"
    conda:
        "envs/python.yaml"
    params:
        annot_file = config["pgRNA_counts_ref"], ## do I need os.path.abspath?
        in_dir = pgRNA_counts_dir,
        script = "workflow/scripts/combine_counts.py"
    log:
        "workflow/logs/combine_counts/combine_counts.log"
    shell:
        "python {params.script} {params.annot_file} {params.in_dir} {output} &>{log}"

## add a rule to gzip all files
## - would this mean I have to unzip files if re-running...?
